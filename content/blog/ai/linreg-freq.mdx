---
title: Linear Regression (1) - Frequentists의 방법
date: "2023-04-29"
description: "대표적인 Regression 중 하나인 Linear regression을 Frequentists의 관점에서 해석하자"
tags: ["AI","Mathematics"]
categories: "AI"
---


## 들어가기 앞서...

### 선형회귀?

**Regression**(회귀)은 데이터의 추세를 표현하는 가장 좋은 함수를 찾는 것이다.
다양한 함수를 이용하여 추세를 표현할 수 있겠지만, 일단은 가장 간단하고 쉽지만 아주 강력한 일차함수를 생각하자.
이를 <Highlight>**Linear Regression**</Highlight>이라고 한다.

### Frequentist?

확률을 바라보는 관점은 여러가지가 있지만, 아마 그 중에서 가장 사람들에게 익숙한 것은 Frequentist(빈도주의자) 들의 관점일 것이다.
이들은, 이름처럼 확률을 **어떠한 사건이 일어나는 빈도**로 해석한다.

즉, 총 $n$번의 시행에서 사건 $E$가 $n_E$번 일어났다면, 이 사건이 일어날 확률 ${\rm P}(E)$는
다음과 같다고 주장하는 것이다.

$$
  {\rm P}(E) = \lim_{n \to \infty} \frac{n_E}{n}
$$

고등학교 때 일명 '통계적 확률'이라고 배운 것이 Frequentist들이 확률을 정의하는 방법이었고, 이는 무척이나 직관적이다.
이렇게 **객관적**인 사건과 시행을 통해 얻은 확률은 매우 **객관적**이며, Frequentist들은 이 **객관성**을 다소 중시한다.

이제는 관점을 데이터로 확장시켜보자. 어떠한 Data 가 parameter $\theta$인 미지의 분포를 따른다고 가정하면,
어떻게 최적의 $\theta$를 구할 수 있을까? 답은 간단하다. Likelihood를 세우고, 이를 최대화 하는 $\theta$를 찾으면 된다.
혹은, 적절한 모델을 세워 관측값과 모델값의 차이를 최소화하는 $\theta$를 찾으면 된다.

이렇게 하면, 결국 최적의 $\theta$ 하나를 구할 수 있을 것이다!

Frequentist의 방법은 후에 소개할 Bayesian의 방법에 비해 훨씬 간단하며, point estimation에 적합하다는 성질을 가지고 있다.
따라서, 일단은 Frequentist의 방법으로 Linear regression model의 최적의 parameter을 찾아보자.

## Simple Linear Regression

### Model

$D$개의 feature을 가지는 $N$개의 Data ${\cal D} = \{ ({\bf x}_i, y_i) \}_{i = 1}^N$ 를 생각하자.
그러면, model은 다음과 같이 설정할 수 있다.

$$
  y = {\bf w}^\top {\bf x} + b
$$

(단, 여기서 ${\bf w} = (w_1, \cdots, w_D)^\top \in \R^D$이고, $b \in \R$ 이다)

여기서 조금만 더 머리를 쓰면, ${\bf w}$ 내에 bias term $b$를 포함시킬 수 있겠다는 생각이 들 것이다.
따라서, 일반성을 잃지 않고, model을 다음과 같이 설정하자.

$$
  y = {\bf w}^\top {\bf x}
$$

### Loss function

Loss function은 여러가지가 있지만, 이번에는 MSE(Minimum Square Error)을 사용할 것이다.
즉, parameter $\bf w$에 대한 loss function $L$은 다음과 같다.

$$
  L({\bf w}) = \frac{1}{2}\sum_{i = 1}^N ({\bf w}^\top {\bf x}_i - y_i)^2
$$

이를 최소화해야 하고 이는 convex function 이므로, 미분하여 0이 되는 값이 최적의 parameter이다.

$$
  \begin{align}
    \dv{L}{\bf w} &= \sum_{i = 1}^N ({\bf w}^\top {\bf x}_i - y_i) {\bf x}_i \\
                  &= \sum_{i = 1}^N {\bf w}^\top {\bf x}_i {\bf x}_i - \sum_{i=1}^N y_i {\bf x}_i  =0
  \end{align}
$$  

여기서 행렬 ${\bf X} \in \R^{D \times N}$, ${\bf y} \in \R^N$을 다음과 같이 정의하면,

$$
  {\bf X} = ({\bf x}_1, \cdots, {\bf x}_N ), \quad {\bf y} = \begin{pmatrix}
    y_1 \\ \vdots \\ y_N
  \end{pmatrix}
$$

다음이 성립한다. (직접 계산해보세용)

$$
\begin{equation}
  \sum_{i=1}^N {\bf x}_i {\bf x}_i^\top = {\bf XX}^\top, \quad \sum_{i = 1}^N y_i{\bf x}_i = {\bf Xy}
\end{equation}
$$

또한, 식 $(2)$의 ${\bf w}^\top {\bf x}_i {\bf x}_i$에서 ${\bf w}^\top {\bf x}_i$ 는 scalar이므로, 뒤로 옮겨도 된다.
그리고, transpose 해도 같으므로 다음이 성립한다.

$$
  \begin{equation}
    {\bf w} ^\top {\bf x}_i {\bf x}_i = {\bf x}_i {\bf w}^\top {\bf x}_i = {\bf x}_i{\bf x}_i^\top {\bf w}
  \end{equation}
$$

따라서, 식 $(2)$에 식 $(3)$과 식 $(4)$를 대입하여 정리하면 다음과 같다.

$$
  \begin{align*}
    \sum_{i = 1}^N {\bf w}^\top {\bf x}_i {\bf x}_i - \sum_{i=1}^N y_i {\bf x}_i &=
    \sum_{i=1}^N {\bf x}_i{\bf x}_i^\top {\bf w} - \sum_{i=1}^N y_i {\bf x}_i \\
    &= {\bf XX^\top w - Xy} = 0
  \end{align*}
$$

그러므로, 미분으로 구한 최적의 parameter ${\bf w^*}$는 다음과 같다.

$$
\begin{equation}
  {\bf w^*} = {\bf (XX^\top)}^{-1}{\bf Xy}
\end{equation}
$$

### Prediction

이제는 구한 ${\bf w}^*$를 바탕으로 새로운 input ${\bf x}$에 대한 output $y$를 추정해보자. \
간단하다. 그냥 linear regression 모델에 대입하기만 하면 된다.

$$
  \begin{align*}
  y &= {\bf w}^{*\top} {\bf x} \\
    &= {\bf x}^\top {\bf w}^* \\
    &= {\bf x}^\top ({\bf XX^\top})^{-1} {\bf Xy}
  \end{align*}
$$

마지막 부분만 다시 써보면, 다음과 같다.

$$
\begin{equation}
  y = {\bf x}^\top ({\bf XX^\top})^{-1} {\bf Xy}
\end{equation}
$$

### Pitfalls!

우리는 방금 closed-form을 구했다. 이는 단 몇 번의 행렬 연산을 통해서 답을 낼 수 있다는 소리로,
굳이 gradient descent같은 반복적인 방법을 사용하지 않아도 된다는 것을 시사한다. 하지만, 이 방법에는 큰 함정이 있다.

바로, $\bf XX^\top$이 항상 invertible 한가? 라는 것이다.

조금만 생각해보면, $N < D$ 일 때는 singular, 즉 invertible하지 않다는 것을 깨달을 수 있다. 즉, 차원에 비해
데이터가 적다면, 이 방법은 전혀 쓸 수 없다. 심지어는, $N$이 $D$보다 아주 약간 크면 ${\bf (XX^\top)}^{-1}$가 매우 커지게 되어서,
추정하는 값이 거의 발산해버린다.

이런 일이 발생하지 않으려면 어떻게 해야할까?

당연히 데이터의 개수를 늘리는 것이 우선이다. 😛

하지만, 여러 이유로 이것이 불가능하다면, 우리의 모델이나 loss function을 고쳐야 할 수 밖에 없다.
일단은, $\bf XX^\top$이 non-singular 하게끔 만들어 주는 것이 목적이므로, 적절한 positive 상수가 곱해진
**identity matrix**를 더하면 된다. 즉, 우리의 목적은 다음 term을 만드는 것이다.

$$
  {\bf XX^\top} + \gamma \bf I
$$
(where $\gamma > 0$)

## With Regularization

저 $\gamma$ 를 만들어주기 위하여, loss function을 약간 손보자. 일명 <Highlight>**regulation term**</Highlight>
이라고 불리는 항을 추가한다.

$$
  L({\bf w}) = \frac{1}{2}\sum_{i = 1}^N ({\bf w}^\top {\bf x}_i - y_i)^2 + \frac{\gamma}{2} \norm{\bf w} ^2
$$

마찬가지로 미분하여 optimal한 parameter을 구해보면, 다음과 같다.

$$
  \begin{align*}
    \dv{L}{{\bf w}} &= { \bf XX^\top w - Xy + \gamma w } \\
                    &= {\bf (XX^\top + \gamma I) w - Xy} = 0
  \end{align*}
$$

따라서, optimal한 parameter과 prediction은 다음과 같다.

$$
  \begin{align}
    {\bf w}^* &= ({\bf XX^\top } + \gamma I) ^{-1} {\bf Xy} \\
    y         &= {\bf x}^\top ({\bf XX^\top } + \gamma I) ^{-1} {\bf Xy}
  \end{align}
$$

이를 식 $(5)$, $(6)$과 비교하면 확실히 regularization term이 포함되어 있어 non-singular 함을 알 수 있다.

## 🤔 Hmm...

근데 뭔가 찝찝하다. Frequentist들은, 그 무엇보다도 객관성을 중시하는데, 여기서 $\gamma$는 결국 주관적으로 결정되는 것이 아닌가?
심지어, 이 $\gamma$가 정확히 어떠한 의미를 가지는지, 어떻게 설정해야 적절한 것인지 알기가 쉽지 않다.

이러한 찝찝한 문제를 주관적 확률의 대명사, Bayesian들은 어떻게 해결할까?
Bayesian의 방법을 따라가면서 이 $\gamma$의 정확한 의미에 대해 알아보자. 다소 힘든 일이 될 것이다 ㅎㅎ.
