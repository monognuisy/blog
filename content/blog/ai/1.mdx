---
title: Differentiation w.r.t. Vector / Matrix
date: "2022-10-30"
description: "함수의 벡터 / 행렬로의 미분. 여러가지 함수를 벡터나 행렬로 미분해보자."
tags: ["AI","Mathematics"]
categories: "AI"
---

## Introduction

간단한 선형회귀 모델을 세워보자. 총 $N$개의 Data $\{({\bf x}_i, y_i)\}_{i = 1}^{i = N}$가 있다고 할 때,
weight vector $\bf w$ 와 bias $b$에 대한 MSE $L$은 다음과 같다.
$$
  L({\bf w}, b) = \sum_{i = 1}^N (y_i - ({\bf w}^\top {\bf x} + b))^2
$$

이를 최소화 하는 $\bf w$의 값을 찾기 위해서는 $L$을 $\bf w$에 대해 미분해야 한다. 이러한 벡터로의 미분, 나아가 행렬로의 미분을 알아보자.

## w.r.t Vector

함수 $f : \R^D \to \R$ 을 생각하자. 즉, $\R^D$의 벡터 ${\bf x} = (x_1, \cdots, x_D)^\top$를 스칼라 $y$로 mapping 하는 함수이다.

이 때, $\dfrac{{\rm d}f}{{\rm d}{\bf x}} $ 는 다음과 같이 정의된다.

$$
  \frac{{\rm d}f}{{\rm d}{\bf x}} := \begin{pmatrix}
    \dfrac{\partial f}{\partial x_1} \\ \\  \vdots \\ \\ \dfrac{\partial f}{\partial x_D}
  \end{pmatrix} \in \R^D
$$

즉, $\bf x$ 각각의 성분에 대하여 편미분한 것을 벡터화한 것이다. 이렇게도 표현할 수 있다.

$$
  \left[ \frac{{\rm d}f}{{\rm d}{\bf x}} \right]_i = \frac{\partial f}{\partial x_D}
$$

여기서 미분의 결과가 원래 벡터와 같은 <b>열벡터(Column vector)</b>임에 유의한다. 이는 마치 $\dfrac{{\rm d}f}{{\rm d}{\bf x}}$ 가
분모(?)에 위치한 $\bf x$를 따라가는 것만 같은 느낌을 준다. 이러한 notation을 **Denominator Layout Notation**이라 한다.

몇몇 책에서는 ${{\rm d}f}/{{\rm d}{\bf x}}$ 를 행벡터로 적는데, 이는 Numerator layout notation을 적용한 결과이다.
많은 인공지능 교재와 Matrix cookbook 등에서는 Denominator layout notation을 사용하므로, 이를 기준으로 설명한다.

### examples

$$
  f({\bf x}) = {\bf w}^\top {\bf x} = w_1 x_1 + \cdots w_D x_D
$$

여기서 ${\partial f}/{\partial x_i} = w_i$ 이므로, ${{\rm d}f}/{{\rm d}{\bf x}}$ 는 다음과 같다.

$$
  \frac{{\rm d}f}{{\rm d}{\bf x}} = \begin{pmatrix}
    w_1 \\ \vdots \\ w_D
  \end{pmatrix} = {\bf w}
$$

---

$$
  f({\bf x}) = \Vert {\bf x} \Vert ^2 = {\bf x}^\top {\bf x}
$$

여기서 ${\partial f}/{\partial x_i} = 2x_i$ 이므로 ${{\rm d}f}/{{\rm d}{\bf x}}$ 는 다음과 같다.

$$
  \frac{{\rm d}f}{{\rm d}{\bf x}} = 2 \begin{pmatrix}
    x_1 \\ \vdots \\ x_D
  \end{pmatrix} = 2{\bf x}
$$

---

$$
\begin{align*}
f({\bf w}) &= \sum _{i = 1}^N ({\bf w}^\top {\bf x}_i - b)^2 \\
&= \sum_{i = 1}^N \left(\sum _{j = 1}^D w_j x_{ij} - b\right)^2
\end{align*}
$$

(단, 여기서 ${\bf x}_i \in \R^D$ 인 vector, $b \in \R$)

$$
\begin{align*}
\pdv{f}{w_1} &= 2\left(\sum_j w_j x_{1j} - b\right) x_{11} + 2 \left(\sum_j w_j x_{2j} - b\right) x_{21} + \cdots \\
&=\sum_{i}^N 2 \left(\sum_j w_j x_{ij} - b\right) x_{i1}
\end{align*}
$$

정리하면,

$$
\dv{f}{\bf w} = \sum_{i}^N 2\left(\sum_j w_j x_{ij} - b\right) \begin{pmatrix}
x_{i1} \\ \vdots \\ x_{iD}
\end{pmatrix} = \sum _{i}^{N} 2({\bf w}^\top {\bf x}_i - b) {\bf x}_i
$$

---

$$
f({\bf w}) = {\bf w} ^\top {\bf X}{\bf w}
$$

(단, 여기서 ${\bf w} \in \R^D$, ${\bf X} \in \R^{D \times D}$)

편의상, 행렬 ${\bf X}$ 의 `(i, j)` index 를 $X_{ij}$ 로 나타내자. 즉, $[{\bf X}]_{ij} = X_{ij}$ 이다.

$$
\begin{align*}
  f({\bf X}) &= {\bf w}^\top{\bf X}{\bf w} \\
             &= \begin{pmatrix}
              w_1 & \cdots & w_D
             \end{pmatrix}
\end{align*}
$$
